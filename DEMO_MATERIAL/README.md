# Running the Demo (A1 Arese)

This guide walks through the full execution of the 4-module pipeline using the **A1 Arese** dataset. It covers setup, data organization, and step-by-step execution.

-----

## 1\. Prerequisites

### A. Download the Data

The dataset is too large to store in the repository.

1.  **Download** the material from OneDrive: **[Download Link](https://www.dropbox.com/scl/fo/y5sw9ob8jjacvnwi7o9j5/ADgXFKlWGZRhwavQNieBwbs?rlkey=lt200bvpot5akey7gr141owfn&st=fo358wi2&dl=0)**.
2.  **Extract** the contents into a folder named `DEMO_MATERIAL` in this directory.
3.  Ensure the `checkpoints` folder contains the model weights (`ERoadNet_85_all.pth`).

### B. Build Docker Images

Ensure all modules are built before running.

```bash
docker build -t 1_data_preprocessing module_1_preprocessing/
docker build -t 2_model_inference_bev_merging module_2_model_inference/
docker build -t 3_graph_generation_and_processing module_3_graph_generation/
docker build -t lanelet2 Lanelet2/
docker build -t 4_lanelet2_conversion module_4_lanelet2_conversion/
```

-----

## 2\. Folder Structure Verification

Before running the commands, ensure your `DEMO_MATERIAL` folder looks exactly like this.

```text
DEMO_MATERIAL/
│
├── a1_arese/               ← Input: Folder containing raw camera images
│
├── checkpoints/            ← Input: Model weights
│   └── ERoadNet_85_all.pth
│
├── custom_model/           ← Optional: Custom model scripts (if testing new models)
│   ├── TorchvisionModel.py
│   └── deeplabv3_resnet50...pth
│
├── gps.csv                 ← Input: Raw GPS data (x, y, z, heading, roll, pitch)
│
├── demo_a1_arese.json      ← Config: The pipeline configuration file
│
└── output/                 ← Output: Automatically created at runtime
    ├── PixelMap/           (Generated by Step 2)
    ├── GraphMap/           (Generated by Step 3)
    └── ...
```

**Key Concept:**

  * We mount `DEMO_MATERIAL` (Host) $\to$ `/app/input` (Container).
  * The configuration file `demo_a1_arese.json` is pre-configured to look for files in `/app/input`.

### Configuring Camera Calibration

If you use your own dataset, you **must update the camera intrinsics and extrinsics** in the configuration file (`demo_a1_arese.json`) to match the physical camera used for recording and its mounting position.

Locate the `bev` section and update the `cameraData` block within `parameters`.

**JSON Path:** `bev` $\to$ `camera` $\to$ `parameters` $\to$ `cameraData`

```json
"bev": {
  "camera": {
    "name": "Camera", // The camera model class name
    "parameters": {
      "cameraData": {
        "intrinsic": {
          "fx": 350.975,  // Focal length x (pixels)
          "fy": 350.975,  // Focal length y (pixels)
          "cx": 335.952,  // Principal point x (image center width)
          "cy": 194.081   // Principal point y (image center height)
        },
        "extrinsic": {
          "x": 0,         // Position X relative to vehicle center (meters)
          "y": 0.06,      // Position Y relative to vehicle center (meters)
          "z": 1.55,      // Position Z relative to vehicle center (meters)
          "yaw": 0,       // Orientation Yaw (radians or degrees, project dependent)
          "pitch": 0.1125, // Orientation Pitch
          "roll": 0       // Orientation Roll
        }
      }
    }
  },
  "view_size": [0, 10, -10, 10],
  "offset_angle": -90,
  "resolution": 0.05,
  "center_of_rotation": [200, 200]
}
```

#### Parameter Breakdown

| Category | Description |
| :--- | :--- |
| **`intrinsic`** | Defines the internal properties of the camera lens (focal length, optical center). These are usually found in the camera's calibration file. |
| **`extrinsic`** | Defines the camera's **pose** (position and orientation) relative to a world or vehicle frame of reference. The vehicle center is usually the origin (0, 0, 0). |

-----

## 3\. Execution: Standard Modules (Steps 1-3)

These modules share the same mounting logic. Run them sequentially.

### Step 1: Data Preprocessing

Aligns images with GPS data using interpolation.

```bash
docker run --rm \
  -v $(pwd)/DEMO_MATERIAL:/app/input \
  -v $(pwd)/DEMO_MATERIAL/output:/app/output \
  1_data_preprocessing \
  -i /app/input/demo_a1_arese.json
```

> **Result:** Generates `/app/input/interpolated_gps.csv`.

### Step 2: Model Inference & BEV Merging

Runs the segmentation model and creates the Bird's Eye View map.

```bash
docker run --rm \
  -v $(pwd)/DEMO_MATERIAL:/app/input \
  -v $(pwd)/DEMO_MATERIAL/output:/app/output \
  -v $(pwd)/DEMO_MATERIAL/custom_model:/app/custom_models \
  2_model_inference_bev_merging \
  -i /app/input/demo_a1_arese.json
```

> **Result:** Populates `output/PixelMap` and `output/Images`.

\<details\>
\<summary\>\<strong\>Advanced: Testing a Custom Model\</strong\>\</summary\>

If you want to use the code in `custom_model/` instead of the default model, update your `demo_a1_arese.json` with this block:

```json
"model": {
    "path": "/app/custom_models",
    "name": "TorchvisionModel",
    "parameters": {
      "pretrained": true,
      "num_classes": 12,
      "checkpoint_path": "/app/custom_models/deeplabv3_resnet50_coco-cd0a2569.pth"
    },
    "num_classes": 12,
    "predict_size": [384, 640]
}
```

\</details\>

### Step 3: Graph Generation

Extracts the road network graph from the PixelMap.

```bash
docker run --rm \
  -v $(pwd)/DEMO_MATERIAL:/app/input \
  -v $(pwd)/DEMO_MATERIAL/output:/app/output \
  3_graph_generation_and_processing \
  -i /app/input/demo_a1_arese.json
```

> **Result:** Populates `output/GraphMap`.

-----

## 4\. Execution: Lanelet2 Conversion (Step 4)

**⚠️ Important Path Change:**
The Lanelet2 container uses a different internal user path (`/home/developer/...`). Note the change in the `-v` flag below.

```bash
docker run --rm \
  -v $(pwd)/DEMO_MATERIAL:/home/developer/material/demo \
  4_lanelet2_conversion \
  script/graph2let.py -i /home/developer/material/demo/demo_a1_arese.json
```

**Final Output:**
You will find the final maps in your local folder:

  * `DEMO_MATERIAL/output/lanenet_output/lanelet2_map.osm`
  * `DEMO_MATERIAL/output/lanenet_output/lanelet2_mapinfo.json`

-----

## Troubleshooting

If a module fails, you can inspect the container internals by running it in interactive mode with a `bash` entrypoint:

```bash
# Example for Step 2
docker run -it --entrypoint bash \
  -v $(pwd)/DEMO_MATERIAL:/app/input \
  -v $(pwd)/DEMO_MATERIAL/output:/app/output \
  2_model_inference_bev_merging
```

Once inside, you can check if files exist using `ls -l /app/input`.